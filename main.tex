\documentclass{article}

\usepackage{times}

\usepackage{graphicx}

\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,latexsym,color}
\usepackage{mdwlist}
\usepackage{todonotes}
%\usepackage{fullpage}
%\usepackage{charter}

\usepackage{url}

\usepackage{wrapfig}

%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}

%\usepackage{fullpage}
\usepackage{marvosym}
%\usepackage[techreport]{systems-cover}
\usepackage{natbib}
\usepackage{graphicx}
\graphicspath{ {Figures/} }
\usepackage{subcaption}
%\captionsetup[subfigure]{justification=justified,singlelinecheck=false}

\newcommand{\R}{\mathsf{R}}
\newcommand{\sgn}[1]{\mbox{sgn}(#1)}
\renewcommand{\vec}[1]{\mathbf{#1}}

\def\a{{\bf a}}
\def\g{{\bf g}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\w{{\bf w}}
\def\v{{\bf v}}
\def\E{\mathbb{E}}
\def\rrow{r_\mathrm{row}}

\newcommand{\err}{\ensuremath{\mathrm{err}}}
\newcommand{\setX}{\Omega}
\newcommand{\setI}{\mathcal{I}}
\newcommand{\OPT}{\ensuremath{\mathrm{OPT}}}

\def\Ji{Ji's comment}

\DeclareMathOperator*{\argmin}{argmin}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}

%\newcommand{\todo}[1]{\noindent \textbf{[TODO:] #1 } }
\usepackage{mathtools}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\date{}

%\title{Training Generalized Linear Models with End-to-end Low Precision:\\
%The Cans, the Cannots, and a Little Bit of Deep Learning
%}
\usepackage{icml2017}

\icmltitlerunning{Training Models with End-to-end Low Precision}

\begin{document}

\twocolumn[
\icmltitle{Training Models with End-to-end Low Precision:\\
The Cans, the Cannots, and a Little Bit of Deep Learning}
    \vskip 0.3in
]


\begin{abstract}

\vspace{-0.75em}
There has recently been significant interest in training 
machine-learning models at low precision: by reducing 
precision, one can reduce computation, communication, and power 
consumption by one order of magnitude. 
We examine training at reduced precision both from a theoretical and practical 
perspective, and ask: 
{\em is it possible to \emph{train} models at end-to-end low 
precision with \emph{provable} guarantees?  can this 
lead to consistent order-of-magnitude speedups?}

\vspace{-0.1em}
For linear models, the answer is yes. We develop a simple 
framework based on one simple, but novel strategy, called double sampling. 
Our framework is able 
to execute training at low precision with no bias, 
guaranteeing convergence, whereas naive quantization 
would introduce significant bias. We validate our framework   
across a range of applications, and show that it enables an 
FPGA prototype that is up to $6.5\times$ faster 
than an implementation using full 32-bits precision.

\vspace{-0.1em}
We further develop a variance-optimal 
stochastic quantization
strategy and show that 
it can make a significant difference in a variety of settings. 
When applied to linear models together with 
double sampling, we save up to another 
$1.7\times$ in data movement.
%compared with uniform quantization. 
When
training deep networks with quantized models, 
we achieve higher accuracy than the state-of-the-art XNOR-Net. 

\vspace{-0.1em}
Last, we extend our framework through approximation to non-linear 
models, such as SVM. We show that, although the bias of using 
low precision data cannot be avoided, 
\textcolor{red}{
our approach can appropriately 
control the bias.} We find in practice {\em 8 bit} 
precision is often sufficient to converge to the correct solution. 
Interestingly, however, in practice, we notice that our framework does not always outperform the naive rounding approach. We discuss this negative result in detail. 


\end{abstract}

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{Figures/RSHighlight}    
\vspace{-2em}
\caption{Overview of theoretical results and
highlights of empirical results. See
Introduction for details.}
\vspace{-1em}
\label{fig:highlight}
\end{figure}

\vspace{-3em}
\section{Introduction}

\vspace{-1em}
The computational cost and power consumption of today's machine learning systems are often driven by data movement, and by the precision of computation. 
In our experience, in applications such as tomographic reconstruction, anomaly detection in mobile sensor networks,
and compressive sensing, the overhead of transmitting the data samples can be massive, 
and hence performance can hinge on reducing the precision of data representation and 
associated computation. 
A similar trend is observed in deep learning, where impressive progress has been reported with systems 
using end-to-end reduced-precision representations~\cite{hubara2016quantized,
rastegari2016xnor,zhou2016dorefa,miyashita2016convolutional}. 
In this context, the motivating question behind our work is:  {\em When training general machine learning models,
can we lower the precision of data representation,
communication, and computation, while maintaining provable guarantees?}
 
% 
%The total operating costs
%or power consumptions of 
%today's machine learning systems 
%are often bounded by data 
%movement and precision of computation.
%From our experience in supporting
%multiple industry partners with 
%a diverse set of applications
%such as tomographic reconstruction,
%anomaly detection over mobile 
%sensor networks, compressive sensing,
%or Ridge regression over
%enterprise data, many of these 
%applications hinge on one question: {\em
%When training generalized linear models,
%can we lower the precision of data representation,
%communication, and computation?}
%
%On the other hand, the Deep Learning
%community has recently witnessed a
%tremendous success in  
%end-to-end low-precision, sometimes as
%low as a single bit, training of neural networks
%with negligible quality loss in many cases~\cite{XXX,XXX,XXX,XXX,
%XXX,XXX,XXX,XXX,XXX}. {\em Can we apply
%these techniques to simpler generalized
%linear models?}

In this paper, we develop a general 
framework to answer this question, and
present both  positive and negative results
 obtained in the context of this framework. 
 Figure~\ref{fig:highlight} encapsulates our results: 
(a) for linear models, we are able to lower the precision of both computation and communication, including input samples, gradients, and model, by up to $16$ times, while still providing rigorous theoretical guarantees; 
(b) our FPGA implementation of this framework achieves up to $6.5\times$ speedup compared with
a 32-bit FPGA implementation, or with a 10-cores CPU running Hogwild!;  
(c) we are able to decrease data movement by $2.7\times$ for
tomographic reconstruction, while getting negligible quality decrease. 
Elements of our framework generalize to (d) non-linear models and  (e) model compression for training deep learning models. 
In the following, we describe our technical contributions in more detail. 


%In this paper, we develop a general 
%framework to answer these questions, and
%present both the positive and negative results
%we obtained. Figure~\ref{fig:highlight} encapsulates our results: 
%(a) for linear
%models, we are able to lower the precision of computation and communication, including input samples, gradients, and model, by up to $16$ times, while still providing rigorous theoretical guarantees 
%we are able to lower the precision of all
%computation and communication involving
%input samples, gradients, and models, by up to 
%$16\times$; (b) when implemented on FPGA, we achieve
%up to $10\times$ speed up compared with
%a 32-bit FPGA implementation and a 10 cores
%CPU running Hogwild!; and (c) we are able to
%decreases data movement by 3$\times$ for
%tomographic reconstruction while getting
%innegligible quality decrease. Our framework
%also applies to (d) non-linear models and 
%(e) model compression for training deep
%learning models. We now summarize our
%technical contributions.

\vspace{-1em}
\subsection{Summary of Technical Contributions}
\vspace{-0.5em}

We consider the following problem in training generialized linear models: 
\begin{align}
\min_{\x}:\quad {1\over 2K}\sum_{k=1}^K l(\a_k^\top \x, b_k)^2 + R(\x)
\label{eqn:leastsquares}
\end{align}
where $l(\cdot,\cdot)$ is a loss function, and $R$ is a regularization term, which could be $\ell_1$ norm, $\ell_2$ norm, or even an indicator function representing the constraint. 
The gradient at the sample $(\a_k, b_k)$ is: 
\[
\g_k := \a_k \frac{\partial l(\a_k^\top \x, b_k)}{\partial \a_k^\top \x} 
\]
We denote the problem dimension by $n$. 
We consider the properties of the algorithm when a lossy compression scheme is applied to the data (samples), 
gradient, and model, to reduce the communication cost of the algorithm---that is, we consider quantization functions $Q_g$, $Q_m$, and $Q_s$, for gradient, model and samples, respectively, in the gradient update:
\begin{align}
\x_{t + 1} \leftarrow \text{prox}_{\gamma R(\cdot)}\left(\x_t - \gamma Q_g(\g_k (Q_m(\x_t), Q_s(\vec{a}_t)))\right).
\label{eq:proxupdate}
\end{align}
where the proximal operator is defined as
\[
\text{prox}_{\gamma R(\cdot)}(\y) =\argmin_{\x} {1\over 2}\|\x-\y\|^2 + \gamma R(\x).
\]

\vspace{-1.5em}
\paragraph{Our Results} We summarize our results as follows. The {\bf (+)}
sign denotes a ``positive result,'' where we achieve
significant practical speedup, and {\bf (--)} otherwise.

\vspace{-1em}
\paragraph{(+) Linear Models.} When $l(\cdot,\cdot)$ is 
the least squares loss, we first notice that
simply doing stochastic quantization of data samples  
(i.e., $Q_s$) introduces bias of the gradient
estimator and therefore SGD would converge
to a different solution. We propose a simple
solution to this problem, by introducing a
{\em double sampling} strategy
$\tilde{Q}_s$ that uses multiple samples to
eliminate the correlation of samples introduced
by the non-linearity of the gradient. We
analyze the additional variance introduced
by double sampling, we find that its impact is \emph{negligible in terms of convergence time} as long as the 
number of bits to store a quantized sample is at least $\Theta( \log n / \sigma )$, 
where $\sigma^2$ is the variance of the standard stochastic gradient. 
This implies that the 32 bit precision may be excessive for many practical scenarios. 

\vspace{-0.5em}
We build on this quantization strategy to obtain an \emph{end-to-end quantization} strategy
for linear models, which compresses all data movements. 
For certain settings of parameters, end-to-end quantization adds as little as a \emph{constant factor} to the variance of the entire process. 

\vspace{-1.5em}
\paragraph{(+) Optimal Quantization and Extension to Deep Learning.}
We then focus on reducing the variance of  
stochastic quantization. We notice that different ways of setting the quantization points have different variances---the standard uniformly-distributed quantization strategy is far from optimal in many settings.
We formulate this as an independent optimization problem, and solve it optimally with 
an efficient dynamic programming algorithm 
that only need to scan the data in a single pass.
When applied to linear models, this optimal 
strategy can save up to $1.6\times$ communication
compared with the uniform strategy.

\vspace{-0.5em}
We perform an analysis of the optimal quantizations for various settings, and observe that the uniform quantization approach
popularly used by state-of-the-art end-to-end
low-precision deep learning training systems
when more than 1 bit is used is suboptimal.
We apply optimal quantization to 
model quantization and shows that, with one
standard neural network, we outperforms the
uniform quantization used by XNOR-Net and a
range of other recent approaches. This
is related, but different, to recent work 
on model compression for inference~\cite{Han:2016:ICLR}. 
To our best knowledge, this is the first time such optimal quantization strategies have been applied to training. 

\vspace{-1.5em}
\paragraph{(--) Non-Linear Models.} We extend our
results to non-linear models such as SVM. We observe that 
we can stretch our multiple-sampling strategy to provide 
unbiased estimators for any polynomials, at the cost of increased variance. 
Building further, we employ Chebyshev polynomials to   
\textcolor{red}{
approximate the gradient of \emph{arbitrary smooth loss functions} within arbitrarily low bias, 
and to provide bounds on the error of an SGD solution obtained from low precision samples. }
%Given the user's error tolerance as input, we can set the bit complexity to lower the bias in order to meet the error threshold. 
%
%By increasing the bit complexity of the representation, we can lower the bias 
%
%This provides a way to control the bias given 
%users' error tolerance as input.  
%When data is separable,
%we prove that \textcolor{red}{BLAH BLAH 
%BLAH BLAH BLAH BLAH BLAH BLAH BLAH BLAH BLAH 
%BLAH BLAH BLAH}. 
We also develop machinery for quantizing SGD for SVM with provable guarantees, even though SVM is not smooth.
\textcolor{red}{In particular, this implies that none of the machinery developed previously applies, for fundamental reasons.}
However, we show that under reasonable assumptions, this can either be ignored, or we can remove its influence using a sublinear number of additional bits, via ideas from streaming and dimensionality reduction.

\vspace{-0.5em}
In practice, using this technique, we are
able to go as low as 8 bit precision for SVM and logistic regression, \textcolor{red}{while providing 
rigorous theoretical guarantees on the error.} However, we notice that the strawman approach, which just
does naive stochastic rounding over the input data to 8bit precision, converges to similar results, 
without the added complexity. 
This negative result is explained by the fact that, to approximate non-linearities such as the step function or the sigmoid well, our framework needs both high degree Chebyshev polynomials, and relatively large samples. 



\vspace{-0.5em}
\section{Linear Models}

\input{linear_model_Ji.tex}
%\input{linear_model.tex}






%\vspace{-2em}
\section{Optimal Quantization Strategy for Variance Reduction} \label{sec:optimal}

\vspace{-0.5em}
% We now revisit the choice of quantization points and present an optimal strategy to minimize the variance term introduced by quantization.



\input{OQS.tex}

\vspace{-0.5em}
\subsection{Extension to Deep Learning}
\vspace{-0.5em}

In this section, we show that it is possible 
to apply optimal quantization to
training deep neural networks.

\vspace{-0.5em}
\paragraph*{State-of-the-art} We focus on
training deep neural networks with quantized
model. Let $\mathcal{W}$ be the model and 
$l(\mathcal{W})$ be the loss function. State-of-the-art quantized networks,
such as XNOR-Net and QNN, replace $\mathcal{W}$
with the quantized version $Q(\mathcal{W})$, and optimize
for
\[
\min_{\mathcal{W}} l(Q(\mathcal{W})).
\]
With a properly defined 
$\frac{\partial Q}{\partial{\mathcal{W}}}$, we can
apply the standard backprop 
algorithm.

\vspace{-0.5em}
Choosing the quantization function $Q$ is
an important design decision. For 1-bit quantization,
XNOR-Net searches the optimal quantization point. However, for multiple bits,
XNOR-Net, as well as other approaches such as QNN, resort
to uniform quantization.

\vspace{-0.5em}
\paragraph*{Optimal Model Quantization for Deep Learning}

We can apply our optimal quantization strategy 
and use it as the quantization function $Q$
in XNOR-Net. Empirically, this results in 
quality improvement
over with the default {\em multi-bits} quantizer in XNOR-Net. 

\vspace{-0.5em}
In spirit, our approach is similar to the 1-bit quantizer of
XNOR-Net, which is equivalent to our approach when the data
distribution is symmetric---we extend this
to multiple bits in a principled way. Another related work
is the uniform quantization strategy 
in {\em log domain}~\cite{miyashita2016convolutional},
which is similar to our approach when the data distribution
is ``log uniform''. However, our approach does not rely on
any specific assumption of the data distribution.
\citet{Han:2016:ICLR} use $k$-means to
compress the model for {\em inference}~---$k$-means
optimizes for a similar, but different, objective
function than us. In this paper, we 
develop a dynamic
programming algorithm to do optimal quantization efficiently,
and to our best knowledge, we
are the first to apply optimal quantization to
{\em training}.



\input{nonlinear.tex}

%\textcolor{red}{DAN: ADD ONE SENTENCE ABOUT HOW IT WORKS IN PRACTICE?}



\begin{table}[t]
\scriptsize
\centering
\begin{tabular}{crrrr}
\hline
\multicolumn{4}{c}{\bf Regression}\\
Dataset           & Training Set & Testing Set & \# Features  \\
\hline
Synthetic 10   & 10,000        & 10,000       & 10               \\
Synthetic 100  & 10,000        & 10,000       & 100              \\
Synthetic 1000 & 10,000        & 10,000       & 1,000           \\
YearPrediction & 463,715       & 51,630       & 90                  \\
cadata         & 10,000        & 10,640       & 8                   \\
cpusmall       & 6,000         & 2,192        & 12     \\
\hline
\hline
\multicolumn{4}{c}{\bf Classification}\\
Dataset           & Training Set & Testing Set & \# Features \\
\hline
cod-rna        & 59,535        & 271,617      & 8    \\
gisette        & 6,000         & 1,000        & 5,000  \\  
epsilon        & 10,000        & 10,000       & 2,000\\  
\hline
\hline
\multicolumn{4}{c}{\bf Deep Learning}\\
Dataset           & Training Set & Testing Set & \# Features \\
\hline
CIFAR-10        & 50,000        & 10,000      &$32\times 32\times 3$     \\
\hline
\end{tabular}
\caption{Dataset statistics}
\label{table:dataset}
\end{table}

\vspace{-0.5em}
\section{Experiments} \label{sec:exp}

\vspace{-0.5em}
We provide empirical validation of
our framework.

\vspace{-1em}
\paragraph{Experimental Setup} 
Table~\ref{table:dataset} shows the 
datasets we use. 
Unless otherwise noted, we always
use diminishing stepsizes $\alpha/k$
where $k$ is the current number of
epoch. We tune 
$\alpha$ for the full precision
implementation, and use the
same initial step size for 
our low precision 
implementation (Theory and
experiments imply that the low precision
implementation often favors smaller step size. 
Thus we do not tune step sizes for the low precision 
implementation, as this can only make our 
approach better.) 

\vspace{-1em}
\paragraph*{\textcolor{red}{Summary of Experiments in ArXiv Version}}
We only report a very small subset of result
here but summarize those in the arXiv
version: (1) Here we only report
{\bf Synthetic 100} for regression and 
{\bf gisette} for classification---we
other data sets are in the arXiv version; The
arXiv version also discusses (2) different
factors such as the number of features
to low-precision algorithms, (3) FPGA
implementation and design
decisions, and (4) refetching
heuristics.





\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{final-experiments/linearmodel} 
\vspace{-1em}
\caption{Linear models with end-to-end low precision.}
\vspace{-1.5em}
\label{fig:convergence}
\end{figure}

\vspace{-1em}
\subsection{Linear Models}
\vspace{-0.5em}

For linear models, we validate that (1) 
with double sampling, SGD with low
precision converges---in
comparable empirical 
convergence rates---to the same solution
as SGD with full precision; and
(2) implemented on FPGA, our low-precision
prototype achieves significant speed-up
because of the decrease of bandwidth
consumption.

\vspace{-1em}
\paragraph{Convergence}

Figure~\ref{fig:convergence} illustrates
the result of training linear models:
(a) linear
regression and (b) least squares SVMs,
with end-to-end low-precision and 
full precision. For
low precision, we pick the 
smallest number of bits that
result in a smooth convergence
curve. We compare the final 
training loss in both settings 
and the convergence rate.

\vspace{-0.5em}
We see that, for both linear regression 
and least squares SVM,
using 3 or 4-bit is always enough
to converge to the same solution
with comparable convergence rate. 
This validates our prediction that
double-sampling provides an
unbiased estimators of the gradient.
Considering the size of input
samples that we need to read, we
could potentially save 8-10$\times$ 
memory bandwidth compared to using 
32-bit. 

\vspace{-1em}
\paragraph{Speedup}
We implemented our low-precision 
framework on a state-of-the-art 
FPGA platform. We leave the detailed 
implementation to the arXiv verison.
This implementation assumes the input
data is already quantized and
stored in memory (data can be
quantized during the
first epoch.)

\vspace{-0.5em}
Figure~\ref{fig:speedup} illustrates 
the result of (1) our FPGA
implementation with quantized data,
(2) FPGA implementation with 32-bit
data, and (3) Hogwild! running with
10 CPU cores.
Observe that all approaches
converge to the same solution.
FPGA with quantized data converges
6-7$\times$ faster
than FPGA with full precision
or Hogwild!. The FPGA implementation
with full precision is
memory-bandwidth bound, and by using our framework on quantized data, we save 
up to 8$\times$ memory-bandwidth, which
explains the speedup.


\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{final-experiments/linear-fpga} 
\vspace{-1em}
\caption{FPGA implementation of linear models.}
\vspace{-1em}
\label{fig:speedup}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{final-experiments/optimal} 
\vspace{-1em}
\caption{Optimal quantization strategy.}
\vspace{-1em}
\label{fig:optimal}
\end{figure}

\vspace{-1em}
\subsection{Data-Optimal Quantization Strategy}
\vspace{-0.5em}

We validate that, with our data-optimal quantization strategy, we can 
significantly decrease the number of 
bits that double-sampling requires to 
achieve the same convergence.
Figure~\ref{fig:optimal}(a) illustrates
the result of using 3-bit and 5-bit
for uniform quantization and optimal 
quantization on the {\bf YearPrediction}
dataset. We see that,
while uniform quantization needs 5-bit
to converge smoothly, optimal
quantization only needs 3-bit. 
We save almost $1.7\times$ number of 
bits by just allocating quantization points wisely.

\vspace{-1em}
\subsection{Extensions to Deep Learning}
\vspace{-0.5em}

We validate that our data-optimal quantization
strategy can be used in training deep neural
networks. We take Caffe's CIFAR-10 tutorial~\cite{Caffe:CIFAR10}
and compare three different quantization
strategies: (1) Full Precision, (2) XNOR5, 
a XNOR-Net implementation that, follows
the multi-bits strategy in
the original paper, quantizes data into
five uniform levels, and (3)
Optimal5, our quantization strategy with
five optimal quantization levels. As
shown in Figure~\ref{fig:optimal}(b), Optimal5
converges to a significantly lower training 
loss compared with XNOR5. Also
Optimal5 achieves $>$5 points higher testing accuracy over XNOR5.
This illustrates the improvement
one can get by training neural network with
a carefully chosen quantization strategy.



\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{final-experiments/chebyshev} 
\vspace{-1em}
\caption{Non-linear models with Chebyshev approximation.}
\vspace{-1.5em}
\label{fig:chebyshev}
\end{figure}

\vspace{-1em}
\subsection{Non-Linear Models}
\vspace{-0.5em}

We validate that (1) our Chebyshev 
approximation approach is able to
converge to almost the same solution 
with 8-bit precision for both SVM
and logistic regression;
and (2) however, we are able to construct
a strawman with 8-bit deterministic 
rounding or naive stochastic rounding
to achieve the same quality and convergence 
rate.

\vspace{-1em}
\paragraph{Chebyshev Approximations}

Figure~\ref{fig:chebyshev} illustrates
the result of training SVM
and logistic regression 
with Chebyshev approximation. Here,
we use Chebyshev polynomials up to
degree 15 (which requires 16 samples
that can be encoded with 4 extra 
bits). For each sample, the precision
is 4-bit, and therefore, in total
we use 8-bit for each single number
in input samples. We see that 
with our quantization framework,
SGD converges to similar training loss 
with comparable empirical convergence 
rate for both SVM and logistic regression.
We also get no loss in test accuracy.



\vspace{-1em}
\paragraph{Negative Results}

We are able to construct the following,
much simpler strategy that also
uses 8-bit to achieve the same quality
and convergence rate as our
Chebyshev. In practice, as both
strategies incur bias on the result,
we do {\em not} see strong reasons to
use our Chebyshev approximation, thus
we view this as a negative result.
As shown in Figure~\ref{fig:chebyshev},
if we simply round the input samples
to the nearest 8-bit fix point
representation (or do rounding
stochastically), we achieve the
same, and sometimes better,
convergence than our Chebyshev 
approximation.

\vspace{-1em}
\section{Related Work} 

\vspace{-0.5em}
There are work on ``low precision SGD''~\cite{DeSa:NIPS:2015,Alistarh:2016:ArXiv}. 
Most of them provide
theoretical guarantee when the gradients are quantized.
The model and input samples, on the other hand, are much more difficult
to analyze because of the non-linearity. In this paper, 
we
focus on the {\em end-to-end}
quantization for all components.

\vspace{-1em}
\paragraph{Low-Precision Deep learning.}

Low-precision training of deep neural networks has been studied
intensively and many heuristics work well for a subset of networks.
OneBit SGD~\cite{Frank:2014:Interspeech} provides
a gradient compression heuristic developed in the context of deep 
neural networks for speech recognition. There are successful 
application of end-to-end quantization to training neural networks
resulting in little to no quality loss~\cite{hubara2016quantized,
rastegari2016xnor,zhou2016dorefa,miyashita2016convolutional,li2016ternary,gupta2015deep}. These work quantize weights, activations, and gradients 
to low-precision (e.g., 1-bit) and revise the backpropagation 
algorithm to be aware of the quantization function.
The empirical success of these work inspired this paper, in which we try
to provide a {\em theoretical} understanding of end-to-end low-precision
training for machine learning models.
Another line of research concerns about inference and model
compression of a pre-trained model~\cite{vanhoucke2011improving,gong2014compressing,Han:2016:ICLR,lin2016fixed,kim2016bitwise,kim2015compression,wu2016quantized}.
In this paper, we focus on training and leave the study of
inference as future work.

\vspace{-1.2em}
\paragraph{Low-Precision Linear Models.}

Quantization is a fundamental topic studied by the
DSP community, and there have been research related to
linear regression models in the presence of quantization
error or other type of noises. For example,
\citet{Gopi:2013:ICML} studied compressive sensing
with \textcolor{red}{binary} quantized measurement, and \textcolor{red}{a two stage algorithm was proposed to recover the sparse high precision solution up to a scale factor.}
%algorithm focuses on getting a high-precision solution with low-precision computation. 
Also, the
classic errors-in-variable model~\cite{Hall:2008:Book}
could also be relevant if quantization is treated 
as a source of ``error''. In this paper, we scope
ourselves to the context of stochastic gradient descent, 
and our insights go beyond simple linear models.

\vspace{-1.2em}
\paragraph{Other Related Work.} Precision of data
representation is a key design decision
for configurable hardwares such as FPGA. There are attempts to
lower the precision when training machine learning models
on these hardware~\cite{Kim:2011:ICASSP}. 
These results are mostly empirical; we
aim at providing a theoretical understanding, which 
enables new algorithms that are not covered 
by these references.

\vspace{-1em}
\section{Discussion and Future Work}
\label{sec:conclusions}

\vspace{-0.5em}
The motivating question of this paper was whether end-to-end low-precision data representation can enable efficient computation, while maintaining convergence guarantees. 
We have shown that a relatively simple stochastic quantization framework can achieve this for linear models. 
Moreover, for this setting, as little as two bits per model dimension are sufficient for good accuracy, and can enable a high-performance FPGA implementation.  

\vspace{-0.3em}
For non-linear models, the picture we observe is more nuanced. 
In particular, we find that our framework can be generalized to this setting, and that in practice \emph{8 bit precision is sufficient} to achieve good accuracy on a variety of tasks, such as SVM and logistic regression. 
However, in this setting, naive rounding has similar performance. 

\vspace{-0.3em}
It is interesting to consider the rationale behind these results. On one hand, our framework is based on the idea of \emph{unbiased approximation} of the original SGD process. For linear models, this is easy to achieve. For non-linear models, this is harder, and we must focus on guaranteeing arbitrarily low bias. 
However, for a variety of interesting functions, such as hinge loss, guaranteeing low bias requires high polynomial approximations. In turn, these increase the variance (since variance bounds are exponential in the degree). These two constraints force us to increase the \emph{density} of the quantization, in order to achieve good approximation guarantees. 

%In future work, we plan to examine compressed data representations which are not necessarily (approximate) unbiased estimators of the high-precision process. 
%Such representations, for instance based on naive rounding with feedback error correction~\cite{1BitSGD}, have been successfully employed for gradient compression, although they currently are not known not provide any guarantees. Since the resulting processes no longer approximate standard SGD, the analysis appears non-trivial. 

%An independent contribution of our work is a set of algorithms for finding an optimal quantization with respect to variance. 
%We show that improved quantizations can significantly improve accuracy for training in both convex and non-convex settings. 
%In future work, we plan to explore the potential of these techniques for training and inference.  

\cleardoublepage





\bibliographystyle{icml2017}
\bibliography{low-precision.bib}


\end{document}
